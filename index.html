<head>
    <title>Yanze Wu</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Yanze Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Yanze Wu">
    <!-- <meta property="og:image" content="files/image.jpg"> -->
    <!-- 	<meta name="twitter:card" content="summary_large_image"> -->
    <link rel="apple-touch-icon" href="files/uts-logo.png">
    <link rel="icon" type="image/png" href="files/uts-logo.png">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <!-- <div class="header-profile-picture"></div> -->
        <div class="header-text">
            <div class="header-name">
                <h1>Yanze Wu</h1>
            </div>
            <div class="header-subtitle">
                Researcher @ ByteDance
            </div>
            <div class="header-links">
                <a class="btn" href="https://github.com/ToTheBeginning">GitHub</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=FdHiVvkAAAAJ&hl=zh-CN">Google Scholar</a>
                /
                <a class="btn" href="#contact">Email</a>
            </div>
        </div>
    </div>
</div>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I am currently a Researcher at ByteDance Intelligent Creation Lab (since August 2023), focusing on AIGC
            topics.
            Prior to joining ByteDance,
            I spent over three years at <a href="https://arc.tencent.com/en/index">Tencent ARC Lab</a>, where I worked
            closely with <a href="https://xinntao.github.io/">Dr. Xintao Wang</a> and <a
                href="https://yu-li.github.io/">Dr. Yu
                Li</a>. I obtained my master's degree from Fudan University in 2020.
        </p>
        <p>
            <font color="#FF8C00">We are looking for research interns at ByteDance Intelligent Creation Lab to
                work on visual content
                creation topics. Feel
                free to contact me if you are interested! </font>
        </p>
    </div>
    <div>
        <h2 class="noselect">Research interest</h2>
        <p>
            My research interests include computer vision and deep learning, particularly focusing on image/video
            restoration and enhancement, generation and editing, <i>etc</i>.
        </p>
    </div>

    <div>
        <h2 class="noselect">News</h2>
        <ul>
            <li>
                [12/2023] One paper was accepted by AAAI 2024.
            </li>
            <li>
                [09/2022] One paper was accepted by NeurIPS 2022.
            </li>
            <li>
                [07/2022] One paper was accepted by ECCV 2022.
            </li>
            <li>
                [07/2021] One paper was accepted by ICCV 2021.
            </li>
        </ul>
    </div>

    <div>
        <h2 class="noselect">Publications</h2>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/t2i-adapter-car.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2302.08453">T2I-Adapter: Learning Adapters
                    to Dig out More Controllable Ability for Text-to-Image Diffusion Models</a><br />
                Chong Mou, Xintao Wang, Liangbin Xie, <span class="bold">Yanze Wu</span>, Jian Zhang, Zhongang Qi, Ying
                Shan, Xiaohu Qie<br />
                <span class="italic">Annual AAAI Conference on Artificial Intelligence (<a
                        href="https://aaai.org/aaai-conference/">AAAI</a>)</span>, 2024<br />
                <a class="btn btn-red" href="https://arxiv.org/abs/2302.08453">Paper (arXiv)</a>
                / <a class="btn" href="https://github.com/TencentARC/T2I-Adapter">Codes (GitHub)</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/animesr.jpg);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2206.07038">AnimeSR: Learning Real-World
                    Super-Resolution Models for Animation Videos</a><br />
                <span class="bold">Yanze Wu*</span>, Xintao Wang*, Gen Li, Ying Shan<br />
                <span class="italic">Advances in Neural Information Processing Systems (<a
                        href="https://nips.cc/Conferences/2022">NeurIPS</a>)</span>, 2022<br />
                <a class="btn btn-red" href="https://arxiv.org/abs/2206.07038">Paper (arXiv)</a>
                / <a class="btn" href="https://github.com/TencentARC/AnimeSR">Codes (GitHub)</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/MM-RealSR.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2205.05065">MM-RealSR: Metric Learning
                    based Interactive Modulation for Real-World Super-Resolution</a><br />
                Chong Mou, <span class="bold">Yanze Wu</span>, Xintao Wang, Chao Dong, Jian Zhang, Ying Shan<br />
                <span class="italic">European Conference on Computer Vision (<a
                        href="https://eccv2022.ecva.net/">ECCV</a>)</span>, 2022 <br />
                <a class="btn btn-red" href="https://arxiv.org/abs/2205.05065">Paper (arXiv)</a>
                / <a class="btn" href="https://github.com/TencentARC/MM-RealSR">Codes (GitHub)</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(images/colorization.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2108.08826">Towards
                    Vivid and Diverse Image Colorization with Generative Color Prior</a><br />
                <span class="bold">Yanze Wu</span>, Xintao Wang, Yu Li, Honglun Zhang, Xun Zhao, Ying Shan<br />
                <span class="italic">IEEE International Conference on Computer Vision (<a
                        href="https://iccv2021.thecvf.com/home/">ICCV</a>)</span>, 2021 <br />
                <a class="btn btn-red" href="https://arxiv.org/abs/2108.08826">Paper (arXiv)</a>
                / <a class="btn" href="https://github.com/ToTheBeginning/GCP-Colorization">Codes (GitHub)</a>
            </div>
        </div>
    </div>

    <div>
        <a id="honor"></a>
        <h2>Awards</h2>
        <p>
            Silver Medal of The 2015 ACM/ICPC Asia EC-Final Contest, 2015<br>
            Silver Medal of The 2015 ACM/ICPC Asia Changchun Regional Contest, 2015<br>
            Silver Medal of The 2016 ACM/ICPC Asia Qingdao Regional Contest, 2016<br>
            3rd Place of Hackathon HACKxFDU, 2017<br>
        </p>
    </div>

    <div>
        <a id="service"></a>
        <h2>Academic Services</h2>
        <p>
            Reviewer: CVPR 2023, ICCV 2023
        </p>
    </div>

    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        I can be contacted directly at <span class="bold">wuyanze123</span> [at] <span class="bold">gmail</span>.com. I
        typically respond within a few days.
    </div>
</div>
<div class="footer noselect">
    <div class="footer-content">
        Thanks for the website design from Nicklas Hansen <a href="https://nicklashansen.github.io/">here</a>.
    </div>
</div>